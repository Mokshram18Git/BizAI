# BizAI
An AI-powered business assistant using LLaMA3 + RAG for real-time insights. Retrieves data via Google Search &amp; BeautifulSoup, deployed on Streamlit, and applies Polynomial Regression to forecast revenues &amp; trends for smarter decision-making.

📈 BizAI – Real-Time Business Insights Chatbot

BizAI is an AI-powered business assistant that delivers real-time insights, forecasts, and analytics by combining LLaMA3, retrieval-augmented generation (RAG), and web scraping. It provides companies with smarter decision-making tools through a simple Streamlit interface.

🚀 Features

🔍 Real-Time Web Data – Uses Google Search & BeautifulSoup to scrape live information.

🤖 AI-Powered Q&A – Context-aware answers generated by LLaMA3 with RAG.

📊 Forecasting – Polynomial Regression for sales/revenue trend predictions.

🖥️ Interactive UI – Built on Streamlit for fast, user-friendly interaction.

🔗 Source Transparency – Shows URLs of scraped websites for credibility.

📂 Project Structure
BIZAI/
│-- app.py            # Streamlit frontend app
│-- ollama_rag.py     # LLaMA3 + RAG answer generation logic
│-- predictor.py      # ML model (Polynomial Regression) for forecasts
│-- scraper.py        # Extracts content from URLs
│-- search.py         # Handles Google search queries
│-- utils.py          # Helper functions
│-- requirements.txt  # Python dependencies
│-- venv/             # Virtual environment (ignored in git)
│-- __pycache__/      # Cache files (ignored in git)

🛠️ Tech Stack

Language Model: LLaMA3 via Ollama

Frameworks: Streamlit, LangChain

Scraping: Google Search + BeautifulSoup

Forecasting: Polynomial Regression (scikit-learn)

Backend: Python

⚡ Getting Started
1. Clone the repo
git clone https://github.com/Mokshram18Git/BizAI.git
cd BizAI

2. Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate    # On Windows
source venv/bin/activate # On Mac/Linux

3. Install dependencies
pip install -r requirements.txt

4. Run the app
streamlit run app.py

**Note:** The chatbot may take a couple of minutes to respond.  
This is because it performs multiple steps:  
1. Sending the query to Google Search  
2. Scraping and cleaning data from the top results  
3. Passing the processed content to the LLaMA3 model  
4. Generating a structured, reliable answer  

Since the system relies on live web retrieval and AI analysis, a small delay is expected for more accurate results.

📊 Example Queries

“Latest revenue of Infosys”

“Who is the CEO of TCS?”

“Predict sales for Amazon next year”

“Top competitors of Samsung in 2025”
